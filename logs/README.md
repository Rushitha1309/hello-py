Logs Guide
==========

This folder contains verbose run logs generated by `main.py`. Each execution creates a timestamped log file:

- Naming: `run_YYYYMMDD_HHMMSS.log`
- Example: `run_20251102_225716.log`
- A small example file `sample.log` is included for reference.

What gets logged
----------------
- Full assistant messages
- Tool inputs (as JSON) and tool outputs (as JSON)
- Per-turn boundaries and per-run summaries
- Auto-submission events and results

Log structure
-------------
1) Arguments header
   - First line captures the effective arguments for the run.
   - Example:
```text
Args: model=claude-haiku-4-5-20251001, source=kaggle, sample_n=50000, minority_frac=0.03, max_steps=4, tau=0.75, seed=42, runs=10, conv_steps=10, sequential=True
```

2) Run separator
   - Marks the start of a run. In concurrent mode the label includes `(async)`.
```text
========== Run 1/3 (async) ==========
```

3) Step headers
   - One per assistant→tool turn, up to `conv_steps`.
```text
=== Step 1/5 ===
```

4) Assistant messages
   - Free-form guidance from the model:
```text
Assistant: I'll help you improve minority F1 on the hidden TEST set. Let me start by initializing the imbalance task.
```

5) Tool input / output blocks
   - Every tool call is logged with input JSON and output JSON.
   - Inputs contain only the fields sent to the tool.
   - Outputs include metrics and budget fields when applicable.
```text
Tool input:
{
  "seed": 42,
  "max_steps": 2,
  "tau": 0.7,
  "source": "kaggle",
  "sample_n": 50000
}

Tool output:
{
  "train_counts": {"neg": 34937, "pos": 62},
  "val_baseline_f1": 0.003461128860489883,
  "tau": 0.7,
  "max_steps": 2
}
```

6) Auto-submission events
   - The runner may auto-submit after `sweep_thresholds` or when conversation ends.
```text
Auto-submitting after sweep_thresholds...
Agent submitted answer: False
```

7) Per-run summary
   - A compact roll-up per run (also included in terminal as final aggregate only):
```text
Summary: pass=False, test_f1=0.6875, actions=["smote(ratio=0.50,k=5)", "use_focal(alpha=0.25,gamma=2.0)"]
```

Key fields in outputs
---------------------
- Metrics
  - `precision`, `recall`, `f1`, `tp`, `fp`, `fn` under `val_metrics` or direct output for validation evaluations.
  - `best_threshold` from `sweep_thresholds` and its associated `val_metrics`.
- Budget
  - `steps_used`: how many effectful paid actions have been used so far.
  - `budget_remaining`: `max_steps - steps_used`.
  - `free`: present and true for free tools (`train`, `eval_on_val`, `sweep_thresholds`).
- Actions used
  - `actions_used`: the list of paid actions executed during the run, used in test grading output.

Context and memory in logs
--------------------------
- Per-turn context (within a run): You will see budget reminders appended between turns, e.g.
  ```text
  Reminder: You may use up to 3 paid actions total; remaining: 1 (steps_used=2). Free tools: train, eval_on_val, sweep_thresholds.
  ```
  This reflects the state returned by the `get_budget` tool after each assistant/tool exchange.
- Cross-run context: In sequential runs, the next run’s prompt includes a short history of previously tried paid-action sets to encourage exploration. Session state is not shared; only the prompt hint changes.

Common errors in logs (and meaning)
-----------------------------------
- `paid_action_not_allowed`: After the first turn, the set of paid action TYPES is locked to those already used; attempting a new paid tool returns this error. Use only the locked set or adjust parameters of already selected tools.
- `max_paid_selection_exceeded`: On the first turn, you tried to select more than `max_steps` unique paid tool types.
- `step_budget_exceeded`: You attempted to execute an additional effectful paid action after exhausting the per-run budget (`steps_used >= max_steps`).
- `invalid_ratio` / SMOTE guards: The provided ratio is invalid or would have no effect; the tool returns an error or free no-op.
- API/Rate limit messages: Logged when the Anthropic API returns errors; the runner ends the conversation gracefully.

Interpreting concurrent logs
----------------------------
- In `--sequential` mode, runs are logged in order.
- In concurrent mode (default when `--sequential` is not set), sections may interleave, and you may see repeated step headers from multiple runs progressing at different speeds. Use the run separators (`========== Run i/n (async) ==========`) to follow each run.

Tips
----
- Only the final aggregate results are printed to terminal. Use the log file for detailed per-run inspection.
- If you need cleaner logs, run with `--sequential`.
- Increase `--conv-steps` if the agent needs more turns to finish its plan; budget still limits paid actions.
- `train`, `eval_on_val`, and `sweep_thresholds` are free and do not impact `steps_used`.

Example excerpt (from sample.log)
---------------------------------
```text
========== Run 1/3 (async) ==========
=== Step 1/5 ===
Assistant: I'll help you improve minority F1 on the hidden TEST set.
Tool input:
{
  "seed": 42,
  "max_steps": 2,
  "tau": 0.7,
  "source": "kaggle",
  "sample_n": 50000
}
Tool output:
{
  "train_counts": {"neg": 34937, "pos": 62},
  "val_baseline_f1": 0.003461128860489883,
  "tau": 0.7,
  "max_steps": 2
}
...
Auto-submitting after sweep_thresholds...
Agent submitted answer: False
Summary: pass=False, test_f1=0.6875, actions=[]
```


